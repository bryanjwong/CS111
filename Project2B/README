NAME: Bryan Wong
EMAIL: bryanjwong@g.ucla.edu
ID: 805111517

QUESTION 2.3.1 - CPU time in the basic list implementation:
===========================================================
Where do you believe most of the CPU time is spent in the 1 and 2-thread list tests?
Why do you believe these to be the most expensive parts of the code?
Where do you believe most of the CPU time is being spent in the high-thread spin-lock tests?
Where do you believe most of the CPU time is being spent in the high-thread mutex tests?

In the 1 and 2-thread list tests, most of the CPU time is probably spent on
performing the actual list operations, since each individual thread is much
less likely to be in contention for the lock. As the number of threads is
increased, there will be a significantly higher amount of time spent waiting
for another thread to give up the lock.

In the spin-lock tests, most of the CPU time is being spent spinning as each
thread waits for the lock to be released. In the mutex tests, most of the CPU
time is likely spent on the overheads of context switching and blocking waiting
processes. Since this occurs very frequently with a large amount of threads,
the mutex lock also scales poorly with number of threads.

QUESTION 2.3.2 - Execution Profiling:
=====================================
Where (what lines of code) are consuming most of the CPU time when the spin-lock
version of the list exerciser is run with a large number of threads?
Why does this operation become so expensive with large numbers of threads?

The lines of code that consume the largest part of the CPU time when a large
number of threads are run on the spin-lock version are the
while(__sync_lock_test_and_set(&spinlock, 1) == 1) lines. If the lock belongs
to a different thread, the test and set routine will be repeatedly called
until a CPU interrupt causes a context switch. If a large number of threads are
present, they will all compete for the same resources, causing a large portion
of the time to be spent spinning.

QUESTION 2.3.3 - Mutex Wait Time:
=================================
Look at the average time per operation (vs. # threads) and the average
wait-for-mutex time (vs. #threads). Why does the average lock-wait time rise so
dramatically with the number of contending threads? Why does the completion time
per operation rise (less dramatically) with the number of contending threads?
How is it possible for the wait time per operation to go up faster (or higher)
than the completion time per operation?

The average lock-wait time rises dramatically with the number of contending
threads because only one thread can hold the lock at a time- as we add more and
more threads, we increase the number of threads that will be waiting at any
given time. This decreases the likelihood of any given thread obtaining the
lock once its last user releases it, since there will be several blocked threads
waiting.

The completion time per operation rises less dramatically with the number of
contending threads because despite some blocked threads being stuck waiting for
the lock, there is one thread making progress on its task. While that thread is
performing list operations, each of the blocked threads will be accumulating
its individual wait-for-lock time. This causes an increase in the number of
threads to have a multiplicative effect on wait-for-lock time, which is why it
goes up faster than completion time per operation.

QUESTION 2.3.4 - Performance of Partitioned Lists
=================================================
Explain the change in performance of the synchronized methods as a function of
the number of lists. Should the throughput continue increasing as the number of lists
is further increased? If not, explain why not. It seems reasonable to suggest the
throughput of an N-way partitioned list should be equivalent to the throughput of a
single list with fewer (1/N) threads. Does this appear to be true in the above curves?
If not, explain why not.

The performance of the synchronized methods increases as the number of lists is
increased, as each additional list allows more parallelism to occur. As a result,
the odds of a conflict where two threads are attempting to use the same lock
decreases. Since the SEASnet machines have a large number of cores, having multiple
lists allows for concurrent operations of many threads. However, the throughput
gains will eventually have diminishing returns as the odds of a collision between
two threads decreases and each CPU core is statistically guaranteed to have a
running thread at any given time.

From analyzing my data, it does not appear that the throughput of an N-way
partitioned list is equivalent to the throughput of a single list with 1/N threads.
The list implementation seems to consistently outperform the single list. This is
because using lists enables parallelism, where multiple threads are able to
contribute to the list operations at the same time, leading to a higher
throughput than the single list, unparallelized version.

Included Files:

1) lab2_list.c: a C driver program that runs various SortedList operations with
   		the following options

		--threads=#		 specify number of threads
		--iterations=#		 specify number of iterations
		--yield=[idl]		 specify types of yielding
		--sync=[m/s]		 specify mutex or spin-lock synchronization
		--lists=#		 specify number of sublists

2) Makefile: contains the following targets

   	     	default:		 builds lab2_list
		tests:			 runs specified test cases to generate CSV results
		profile:		 run tests with profiling tools to generate
					 an execution profiling report
		graphs:			 use gnuplot to generate required graphs
		clean:			 delete all programs and output generated
					 by the Makefile

3) SortedList.h: header file for SortedList interface

4) SortedList.c: implementation of the SortedList interface

5) lab2b_list.csv: Test results used for GNUplot graphing

6) profile.out: Execution profiling report showing where time was spent in the
   		un-partitioned spin lock implementation

7) lab2b_1.png: throughput vs. number of threads for mutex and spin-lock synchronized
   		list operations.

8) lab2b_2.png: mean time per mutex wait and mean time per operation for
   		mutex-synchronized list operations.

9) lab2b_3.png: successful iterations vs. threads for each synchronization method.

10) lab2b_4.png: throughput vs. number of threads for mutex synchronized partitioned lists.

11) lab2b_5.png: throughput vs. number of threads for spin-lock-synchronized partitioned lists.

12) lab2b_list.gp: Script that generates GNUplot graphs